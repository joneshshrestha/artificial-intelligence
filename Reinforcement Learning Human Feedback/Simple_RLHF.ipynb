{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnLyRqhbQQUv"
      },
      "source": [
        "## CSC 580 AI II (Winter 2026) **HW\\#5 RLHF** -- Start-up code\n",
        "\n",
        "### **Simple_RLHF.ipynb** -- Simple implementation of Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "### Students fill in the places indicated with \"(*) TODO\" (1-15). Sample outputs are baked in this notebook file but also in the html of this file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29bJ5Pk2hzgL"
      },
      "outputs": [],
      "source": [
        "# ## Code piece to mount my Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\") # my Google Drive root directory will be mapped here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV2p-JvSh0WV"
      },
      "outputs": [],
      "source": [
        "# # Change the working directory to your own work directory (where the code file is).\n",
        "# import os\n",
        "# thisdir = '/content/drive/My Drive/CSC580_Winter2026/hw5_rlhf'\n",
        "# os.chdir(thisdir)\n",
        "\n",
        "# # Ensure the files are there (in the folder)\n",
        "# !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vsHkoW6eOXqa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8N868xPzOlx3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: gpt2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"gpt2\"  # https://huggingface.co/openai-community/gpt2\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # load from HuggingFace\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ7xVV8kRohi"
      },
      "source": [
        "## Part 1: Supervised Fine-Tuning (SFT) of Policy Model\n",
        "\n",
        "We use [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) -- \"an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V_izlkgfR5wR"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "class SFTDataset:\n",
        "    def __init__(self, sample_size=10000):\n",
        "        print(\"Loading SFT dataset...\")\n",
        "        # Use a reasonable-sized instruction dataset\n",
        "        dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "\n",
        "        # Limit to sample_size\n",
        "        sample_size = min(sample_size, len(dataset))\n",
        "        dataset = dataset.select(range(sample_size))\n",
        "\n",
        "        # Format for instruction following\n",
        "        def format_example(example):\n",
        "            return {\n",
        "                'text': f\"Instruction: {example['instruction']}\\nContext: {example['context']}\\nResponse: {example['response']}\"\n",
        "            }\n",
        "\n",
        "        self.dataset = dataset.map(format_example, remove_columns=['instruction', 'context', 'response', 'category'])\n",
        "        print(f\"SFT dataset size: {len(self.dataset)}\")\n",
        "\n",
        "    def get_batch(self, batch_size=BATCH_SIZE):\n",
        "        \"\"\"Get batch for SFT training (by random sampling)\"\"\"\n",
        "        indices = np.random.choice(len(self.dataset), batch_size, replace=False)\n",
        "        texts = [self.dataset[int(i)]['text'] for i in indices]\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VmJY-_pgSZEl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "STEP 1: SUPERVISED FINE-TUNING\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 724.86it/s, Materializing param=transformer.wte.weight]             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SFT dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 15011/15011 [00:00<00:00, 187347.34 examples/s]\n",
            "Map: 100%|██████████| 5000/5000 [00:00<00:00, 21285.42 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SFT dataset size: 5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Step 0/200: Loss=6.2161\n",
            "  Step 50/200: Loss=1.0223\n",
            "  Step 100/200: Loss=1.8549\n",
            "  Step 150/200: Loss=1.2704\n",
            "SFT training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Writing model shards: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n"
          ]
        }
      ],
      "source": [
        "# Function for sft training\n",
        "def train_sft(num_steps=200):\n",
        "    \"\"\"Simple SFT training\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: SUPERVISED FINE-TUNING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load model -- CausalLM model (thus language modelling)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Ensure pad token is set for SFT model config\n",
        "    if model.config.pad_token_id is None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Dataset\n",
        "    dataset = SFTDataset(sample_size=5000)\n",
        "\n",
        "    # Create an optimizer for the model\n",
        "    # Use AdamW Optimizer with learning rate 2e-5\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        batch = dataset.get_batch(batch_size=BATCH_SIZE)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Forward pass -- pass inputs and labels\n",
        "        outputs = model(**batch, labels=batch['input_ids'])\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the losses\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"  Step {step}/{num_steps}: Loss={loss.item():.4f}\")\n",
        "\n",
        "    print(\"SFT training complete!\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(\"./sft_model\")\n",
        "    tokenizer.save_pretrained(\"./sft_model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "#----------------------------\n",
        "# Train SFT model\n",
        "sft_model = train_sft()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIooXfglX4nK"
      },
      "source": [
        "## Part 2: Preference Dataset for Reward Modeling\n",
        "\n",
        "We use [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) -- a dataset containing \"Human preference data about helpfulness and harmlessness\".  It has columns \"chosen\" and \"rejected\".\n",
        "\n",
        "For training, the default size of 8_000 instances, randomly chosen, are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "02XQGfl9Ygtb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading preference dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 160800/160800 [00:01<00:00, 104421.07 examples/s]\n",
            "Generating test split: 100%|██████████| 8552/8552 [00:00<00:00, 93579.04 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preference dataset size: 7731\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "class PreferenceDataset:\n",
        "    def __init__(self, sample_size=10000):\n",
        "        print(\"\\nLoading preference dataset...\")\n",
        "        # Use Anthropic HH-RLHF dataset\n",
        "        dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
        "\n",
        "        # Limit to sample_size\n",
        "        sample_size = min(sample_size, len(dataset))\n",
        "        dataset = dataset.select(range(sample_size))\n",
        "\n",
        "        # Process examples\n",
        "        processed_data = []\n",
        "        for example in dataset:\n",
        "            chosen = example['chosen']\n",
        "            rejected = example['rejected']\n",
        "\n",
        "            # Extract assistant responses (last part after \"Assistant: \")\n",
        "            # The format is: [human message]\\n\\nAssistant: [response]\n",
        "            if \"\\n\\nAssistant: \" in chosen:\n",
        "                chosen_response = chosen.split(\"\\n\\nAssistant: \")[-1] # Get the response part\n",
        "                rejected_response = rejected.split(\"\\n\\nAssistant: \")[-1] # Same for rejected\n",
        "\n",
        "                # Extract prompt (everything before assistant response)\n",
        "                prompt = chosen.split(\"\\n\\nAssistant: \")[0]  # Get the prompt part\n",
        "\n",
        "\n",
        "                if len(chosen_response) > 10 and len(rejected_response) > 10:\n",
        "                    processed_data.append({\n",
        "                        'prompt': prompt,\n",
        "                        'chosen': chosen_response,\n",
        "                        'rejected': rejected_response\n",
        "                    })\n",
        "\n",
        "        self.data = processed_data\n",
        "        print(f\"Preference dataset size: {len(self.data)}\")\n",
        "\n",
        "    def get_batch(self, batch_size=BATCH_SIZE):\n",
        "        \"\"\"Get batch of (prompt, chosen, rejected)\"\"\"\n",
        "        indices = np.random.choice(len(self.data), batch_size, replace=False)\n",
        "        batch = [self.data[int(i)] for i in indices]\n",
        "\n",
        "        # Tokenize\n",
        "        prompts = [item['prompt'] for item in batch]\n",
        "        chosen_texts = [item['chosen'] for item in batch]\n",
        "        rejected_texts = [item['rejected'] for item in batch]\n",
        "\n",
        "        # Tokenize prompts\n",
        "        prompt_tokens = tokenizer(\n",
        "            prompts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH // 2,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Tokenize responses\n",
        "        chosen_tokens = tokenizer(\n",
        "            chosen_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH // 2,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        rejected_tokens = tokenizer(\n",
        "            rejected_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH // 2,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return prompt_tokens, chosen_tokens, rejected_tokens\n",
        "\n",
        "#-------------------------------------------\n",
        "# Create dataset\n",
        "pref_dataset = PreferenceDataset(sample_size=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qigzFCoRYn_B"
      },
      "source": [
        "## Part 3: Reward Model Training\n",
        "\n",
        "Note that input text is first tokenized by the tokenizer, and a tensor of the token ids is created.  The remaining positions after the fixed length of tensor will be filled with pad_tokens, and an attention mask is created based on that (entry values 1 for in-sentences, 0 for padding).\n",
        "\n",
        "For each instance, the reward model processes a pair: a chosen text and a rejected text.  Each one produces a reward score.  We examine the difference between the two scores then... (you figure out the rest).\n",
        "\n",
        "For **training**, the poor model uses 30% of the randomly selected instances, while the good model uses 100% of the instances.\n",
        "\n",
        "For **evaluation**, 30 instances for the poor model and 100 instances for the good models, both randomly chosen, are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nYG3EB2hZIzX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "STEP 2: REWARD MODEL TRAINING\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 726.51it/s, Materializing param=transformer.wte.weight]             \n",
            "\u001b[1mGPT2ForSequenceClassification LOAD REPORT\u001b[0m from: gpt2\n",
            "Key          | Status  | \n",
            "-------------+---------+-\n",
            "score.weight | MISSING | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training POOR reward model...\n",
            "  Step 0/30: Loss=0.6933\n",
            "  Step 20/30: Loss=0.8595\n",
            "Reward model training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 582.50it/s, Materializing param=transformer.wte.weight]             \n",
            "\u001b[1mGPT2ForSequenceClassification LOAD REPORT\u001b[0m from: gpt2\n",
            "Key          | Status  | \n",
            "-------------+---------+-\n",
            "score.weight | MISSING | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training GOOD reward model...\n",
            "  Step 0/100: Loss=1.0007\n",
            "  Step 20/100: Loss=0.7231\n",
            "  Step 40/100: Loss=0.5992\n",
            "  Step 60/100: Loss=1.0879\n",
            "  Step 80/100: Loss=1.1676\n",
            "Reward model training complete!\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Start from the pretrained model from HuggingFace (with classification head)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=1\n",
        "        )\n",
        "\n",
        "        # Ensure pad token is set\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).logits.squeeze(-1)\n",
        "\n",
        "    def compute_loss(self, chosen, rejected):\n",
        "        chosen_rewards = self.forward(chosen['input_ids'], chosen['attention_mask'])\n",
        "        rejected_rewards = self.forward(rejected['input_ids'], rejected['attention_mask'])\n",
        "\n",
        "        # Implement the Bradley-Terry loss\n",
        "        # The loss should encourage chosen_rewards > rejected_rewards\n",
        "        # Use log sigmoid of the difference\n",
        "        diff = chosen_rewards - rejected_rewards\n",
        "\n",
        "        # Compute the Bradley-Terry loss\n",
        "        loss = -F.logsigmoid(diff).mean() # mean over minibatch\n",
        "\n",
        "        # accuracy is when diff is > 0 (and the mean over the list)\n",
        "        accuracy = (diff > 0).float().mean().item()\n",
        "\n",
        "        return loss, accuracy\n",
        "\n",
        "#--------------------\n",
        "def train_reward_model(train_fraction=1.0, name=\"Model\"):\n",
        "    \"\"\"Train reward model with different quality levels\"\"\"\n",
        "    model = RewardModel().to(device)\n",
        "\n",
        "    # Create an optimizer for the reward model\n",
        "    # Use Adam with learning rate 1e-5\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(f\"\\nTraining {name} reward model...\")\n",
        "\n",
        "    model.train()\n",
        "    steps = int(100 * train_fraction)  # Adjust fraction based on desired quality\n",
        "\n",
        "    # training using instances randomly chosen from preference data\n",
        "    for step in range(steps):\n",
        "        _, chosen, rejected = pref_dataset.get_batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Move to device\n",
        "        chosen = {k: v.to(device) for k, v in chosen.items()}\n",
        "        rejected = {k: v.to(device) for k, v in rejected.items()}\n",
        "\n",
        "        # Compute the loss using the reward model's compute_loss method \n",
        "        loss, accuracy = model.compute_loss(chosen, rejected)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"  Step {step}/{steps}: Loss={loss.item():.4f}\")\n",
        "\n",
        "    print(\"Reward model training complete!\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "#--------------------\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 2: REWARD MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "#-----------------------------------------------\n",
        "# Train two reward models with different quality\n",
        "#-----------------------------------------------\n",
        "poor_rm = train_reward_model(train_fraction=0.3, name=\"POOR\")\n",
        "good_rm = train_reward_model(train_fraction=1.0, name=\"GOOD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDGZyX2bZmKU"
      },
      "source": [
        "## 3.1 Evaluate reward models\n",
        "\n",
        "Evaluate each reward model using randomly selected from the preference data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5vnACGnHZt5-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reward Model Evaluation:\n",
            "  Poor RM Accuracy: 0.5375\n",
            "  Good RM Accuracy: 0.5437\n"
          ]
        }
      ],
      "source": [
        "def evaluate_rm(model, num_batches=20):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_batches):\n",
        "            _, chosen, rejected = pref_dataset.get_batch(batch_size=8)\n",
        "\n",
        "            chosen = {k: v.to(device) for k, v in chosen.items()}\n",
        "            rejected = {k: v.to(device) for k, v in rejected.items()}\n",
        "\n",
        "            chosen_rewards = model(chosen['input_ids'], chosen['attention_mask'])\n",
        "            rejected_rewards = model(rejected['input_ids'], rejected['attention_mask'])\n",
        "\n",
        "            # Count how many times chosen_rewards > rejected_rewards\n",
        "            # This is the accuracy metric\n",
        "            correct = (chosen_rewards > rejected_rewards).sum().item()\n",
        "\n",
        "            total_correct += correct\n",
        "            total_samples += len(chosen_rewards)\n",
        "\n",
        "    return total_correct / total_samples\n",
        "\n",
        "poor_acc = evaluate_rm(poor_rm)\n",
        "good_acc = evaluate_rm(good_rm)\n",
        "\n",
        "print(f\"\\nReward Model Evaluation:\")\n",
        "print(f\"  Poor RM Accuracy: {poor_acc:.4f}\")\n",
        "print(f\"  Good RM Accuracy: {good_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNcnnNOAaapT"
      },
      "source": [
        "## Part 4: PPO Implementation\n",
        "\n",
        "Policy model starts with the model that resulted from the supervised fine-tuned (SFT) earlier in Part 1. The algorithm also utilizes a model (another one) as a (frozen) **reference model** to compute KL divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9OvHHfMc5BV"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "class SimplePPO:\n",
        "    def __init__(self, reward_model):\n",
        "        # Start from the Policy model fine-tuned earlier (saved in a subfolder)\n",
        "        self.policy = AutoModelForCausalLM.from_pretrained(\"./sft_model\").to(device)\n",
        "        self.reward_model = reward_model\n",
        "        # reference model as the base model\n",
        "        self.ref_model = AutoModelForCausalLM.from_pretrained(\"./sft_model\").to(device)\n",
        "\n",
        "        # Ensure pad token is set for policy and ref_model configs\n",
        "        if self.policy.config.pad_token_id is None:\n",
        "            self.policy.config.pad_token_id = tokenizer.pad_token_id\n",
        "        if self.ref_model.config.pad_token_id is None:\n",
        "            self.ref_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "        # (*) TODO 9: Freeze the reference model parameters\n",
        "        for param in self.ref_model.parameters():\n",
        "            param.requires_grad = # Fill in your code\n",
        "\n",
        "        # (*) TODO 10: Create an optimizer for the policy model\n",
        "        # Use Adam with learning rate 5e-6\n",
        "        self.optimizer = # Fill in your code\n",
        "\n",
        "        self.rewards_history = []\n",
        "        self.kl_history = []\n",
        "        self.episode_rewards = []  # For tracking per-episode rewards\n",
        "        self.episode_kls = []      # For tracking per-episode KL divergences\n",
        "\n",
        "    def generate_response(self, prompt, max_tokens=64):\n",
        "        \"\"\"Generate response with log probabilities\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Present the (tokenized) input to the model\n",
        "        with torch.no_grad():\n",
        "            outputs = self.policy.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.8,\n",
        "                top_k=40,\n",
        "                top_p=0.9,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                pad_token_id=tokenizer.pad_token_id # Explicitly set pad_token_id\n",
        "            )\n",
        "\n",
        "        # Token ids of the generated response text\n",
        "        response_ids = outputs.sequences\n",
        "\n",
        "        # Get log probabilities\n",
        "        if outputs.scores:\n",
        "            logits = outputs.scores[-1]  # output from the last token only (simplified)\n",
        "            log_probs = F.log_softmax(logits, dim=-1) # log-softmax'ed\n",
        "        else:\n",
        "            log_probs = None\n",
        "\n",
        "        return response_ids, log_probs\n",
        "\n",
        "    def compute_reward(self, response_ids):\n",
        "        \"\"\"Get reward from reward model\"\"\"\n",
        "        attention_mask = torch.ones_like(response_ids)\n",
        "        # reward score as the mean of the tokens in the response\n",
        "        reward = self.reward_model(response_ids, attention_mask).mean()\n",
        "        return reward\n",
        "\n",
        "    def compute_kl(self, response_ids):\n",
        "        \"\"\"Compute KL divergence between policy and reference models\"\"\"\n",
        "        attention_mask = torch.ones_like(response_ids)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get the logits from the reference model\n",
        "            ref_outputs = self.ref_model(input_ids=response_ids, attention_mask=attention_mask, use_cache=False, return_dict=True)\n",
        "            ref_logits = ref_outputs.logits\n",
        "\n",
        "        # Get the logits from the policy model\n",
        "        policy_outputs = self.policy(input_ids=response_ids, attention_mask=attention_mask, use_cache=False, return_dict=True)\n",
        "        policy_logits = policy_outputs.logits\n",
        "\n",
        "        # KL calculation\n",
        "        kl = F.kl_div(\n",
        "            F.log_softmax(policy_logits[:, :-1], dim=-1),\n",
        "            F.softmax(ref_logits[:, :-1], dim=-1),\n",
        "            reduction='batchmean'\n",
        "        )\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def train_step(self, prompt, kl_weight=0.01):\n",
        "        \"\"\"Single PPO step\"\"\"\n",
        "        self.policy.train()\n",
        "\n",
        "        # Generate response (from the policy model)\n",
        "        response_ids, _ = self.generate_response(prompt)\n",
        "\n",
        "        # Compute reward\n",
        "        reward = self.compute_reward(response_ids)\n",
        "\n",
        "        # Compute KL penalty\n",
        "        kl_penalty = self.compute_kl(response_ids)\n",
        "\n",
        "        # (*) TODO 12: Compute the PPO loss\n",
        "        # We want to maximize reward but minimize KL divergence\n",
        "        loss = ????? * reward + kl_weight * kl_penalty # Fill in ?????\n",
        "\n",
        "        # Clip the loss of gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
        "\n",
        "        # (*) TODO 13: Backprop\n",
        "        # Fill in your code\n",
        "\n",
        "        return reward.item(), kl_penalty.item()\n",
        "\n",
        "    def train(self, prompts, num_episodes=80):\n",
        "        \"\"\"Training loop.  NOTE: Ooutput texts are evaluated by the GOOD reward model.\"\"\"\n",
        "        print(f\"\\nTraining PPO with {'POOR' if self.reward_model == poor_rm else 'GOOD'} reward model...\")\n",
        "\n",
        "        # Clear history\n",
        "        self.rewards_history = []\n",
        "        self.kl_history = []\n",
        "        self.episode_rewards = []\n",
        "        self.episode_kls = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            # Sample multiple prompts per episode for better training\n",
        "            num_prompts_per_episode = 3  # Multiple prompts per episode\n",
        "            batch_rewards = []\n",
        "            batch_kls = []\n",
        "\n",
        "            for _ in range(num_prompts_per_episode):\n",
        "                # Select a prompt (tried in this episode)\n",
        "                if episode < len(prompts):\n",
        "                    # Early: go through prompts systematically\n",
        "                    prompt_idx = episode % len(prompts)\n",
        "                    prompt = prompts[prompt_idx]\n",
        "                else:\n",
        "                    # Later: random sampling\n",
        "                    prompt = np.random.choice(prompts)\n",
        "\n",
        "                # Reward and KL divergence score from this prompt\n",
        "                reward, kl = self.train_step(prompt)\n",
        "                batch_rewards.append(reward)\n",
        "                batch_kls.append(kl)\n",
        "\n",
        "            # Average over batch\n",
        "            avg_reward = np.mean(batch_rewards)\n",
        "            avg_kl = np.mean(batch_kls)\n",
        "\n",
        "            # Track metrics\n",
        "            self.episode_rewards.append(avg_reward)\n",
        "            self.episode_kls.append(avg_kl)\n",
        "            self.rewards_history.append(avg_reward)\n",
        "            self.kl_history.append(avg_kl)\n",
        "\n",
        "            # Logging -- Compute the average reward over the last 20 episodes\n",
        "            if (episode + 1) % 20 == 0:\n",
        "                # Compute moving averages\n",
        "                recent_rewards = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards\n",
        "                recent_kls = self.episode_kls[-20:] if len(self.episode_kls) >= 20 else self.episode_kls\n",
        "\n",
        "                avg_recent_reward = np.mean(recent_rewards)\n",
        "                avg_recent_kl = np.mean(recent_kls)\n",
        "\n",
        "                print(f\"  Episode {episode+1}/{num_episodes}: \"\n",
        "                      f\"Reward={avg_reward:.4f} (Avg20={avg_recent_reward:.4f}), \"\n",
        "                      f\"KL={avg_kl:.4f} (Avg20={avg_recent_kl:.4f})\")\n",
        "\n",
        "        return self.episode_rewards, self.episode_kls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YMWEqlmc7Jq"
      },
      "source": [
        "## Part 5: Training Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chcuVAxAdH1H"
      },
      "outputs": [],
      "source": [
        "# Load training prompts\n",
        "import ppo_train_prompts\n",
        "from ppo_train_prompts import training_prompts\n",
        "\n",
        "print(f\"Number of training prompts: {len(training_prompts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcXCQkgEdMx2"
      },
      "source": [
        "## Part 6: Train PPO with Both Reward Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RisYjqpdT6n"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: PPO TRAINING (with enhanced prompts)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use augmented prompts for training\n",
        "ppo_training_prompts = training_prompts\n",
        "\n",
        "print(\"\\n1. Training with POOR reward model...\")\n",
        "ppo_poor = SimplePPO(poor_rm)\n",
        "poor_rewards, poor_kls = ppo_poor.train(ppo_training_prompts, num_episodes=150)\n",
        "\n",
        "print(\"\\n2. Training with GOOD reward model...\")\n",
        "ppo_good = SimplePPO(good_rm)\n",
        "good_rewards, good_kls = ppo_good.train(ppo_training_prompts, num_episodes=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wFprf8bGGny"
      },
      "source": [
        "## Part 7: Evaluation and Comparison\n",
        "\n",
        "Evaluate the two policy models (using poor/good reward models) plus Base and SFT models, using the unseen evaluation prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWGzN4aGGLlj"
      },
      "outputs": [],
      "source": [
        "from ppo_train_prompts import evaluation_prompts\n",
        "\n",
        "def evaluate_policy(policy, reward_model, eval_prompts):\n",
        "    \"\"\"Evaluate policy using reward model\"\"\"\n",
        "    policy.eval()\n",
        "    rewards = []\n",
        "    all_responses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for prompt in eval_prompts:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            output = policy.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.pad_token_id # Explicitly set pad_token_id to suppress warning\n",
        "            )\n",
        "\n",
        "            reward = reward_model(output, torch.ones_like(output)).mean().item()\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Store response for first few prompts\n",
        "            if len(all_responses) < 5:\n",
        "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "                response = response[len(prompt):].strip()\n",
        "                all_responses.append({\n",
        "                    'prompt': prompt[:50] + \"...\" if len(prompt) > 50 else prompt,\n",
        "                    'response': response[:100] + \"...\" if len(response) > 100 else response,\n",
        "                    'reward': reward\n",
        "                })\n",
        "\n",
        "    policy.train()\n",
        "    return {\n",
        "        'mean': np.mean(rewards),\n",
        "        'std': np.std(rewards),\n",
        "        'min': np.min(rewards),\n",
        "        'max': np.max(rewards),\n",
        "        'median': np.median(rewards),\n",
        "        'all_rewards': rewards,\n",
        "        'sample_responses': all_responses\n",
        "    }\n",
        "\n",
        "#-------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL EVALUATION ON ALL 200+ EVALUATION PROMPTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "## Load (and specify) four models for comparison\n",
        "# 1. Base model -- an off-the-chelf LM (distilgpt2)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "# Ensure pad_token_id is set for base_model config\n",
        "if base_model.config.pad_token_id is None:\n",
        "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 2. SFT model -- after fine-tuning the base model (Part 1)\n",
        "sft_model_eval = AutoModelForCausalLM.from_pretrained(\"./sft_model\").to(device)\n",
        "# Ensure pad_token_id is set for sft_model_eval config\n",
        "if sft_model_eval.config.pad_token_id is None:\n",
        "    sft_model_eval.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 3. and 4. are from the cell above (ppo_poor.policy and ppo_good.policy)\n",
        "\n",
        "# Evaluate all models using GOOD reward model using GOOD RM as judge (to give eval scores)\n",
        "print(\"\\nEvaluating models (using GOOD RM as judge):\")\n",
        "\n",
        "# Evaluation\n",
        "base_results = evaluate_policy(base_model, good_rm, evaluation_prompts)\n",
        "sft_results = evaluate_policy(sft_model_eval, good_rm, evaluation_prompts)\n",
        "poor_results = evaluate_policy(ppo_poor.policy, good_rm, evaluation_prompts)\n",
        "good_results = evaluate_policy(ppo_good.policy, good_rm, evaluation_prompts)\n",
        "\n",
        "# Print comprehensive evaluation results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Model':<20} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10} {'Median':<10}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Base Model':<20} {base_results['mean']:.4f}    {base_results['std']:.4f}    {base_results['min']:.4f}    {base_results['max']:.4f}    {base_results['median']:.4f}\")\n",
        "print(f\"{'SFT Model':<20} {sft_results['mean']:.4f}    {sft_results['std']:.4f}    {sft_results['min']:.4f}    {sft_results['max']:.4f}    {sft_results['median']:.4f}\")\n",
        "print(f\"{'Poor RM Policy':<20} {poor_results['mean']:.4f}    {poor_results['std']:.4f}    {poor_results['min']:.4f}    {poor_results['max']:.4f}    {poor_results['median']:.4f}\")\n",
        "print(f\"{'Good RM Policy':<20} {good_results['mean']:.4f}    {good_results['std']:.4f}    {good_results['min']:.4f}    {good_results['max']:.4f}    {good_results['median']:.4f}\")\n",
        "\n",
        "# (*) TODO 14: Calculate percentage improvement over base model on evaluation set\n",
        "print(f\"\\nIMPROVEMENT OVER BASE MODEL (on evaluation set):\")\n",
        "# FILL IN YOUR CODE\n",
        "#\n",
        "#\n",
        "\n",
        "# Show sample responses from evaluation (5 instances in each result)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE RESPONSES FROM EVALUATION PROMPTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nBase Model - Sample Responses:\")\n",
        "for i, sample in enumerate(base_results['sample_responses'], 1):\n",
        "    print(f\"  {i}. Prompt: {sample['prompt']}\")\n",
        "    print(f\"     Response: {sample['response']}\")\n",
        "    print(f\"     Reward: {sample['reward']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nSFT Model - Sample Responses:\")\n",
        "for i, sample in enumerate(sft_results['sample_responses'], 1):\n",
        "    print(f\"  {i}. Prompt: {sample['prompt']}\")\n",
        "    print(f\"     Response: {sample['response']}\")\n",
        "    print(f\"     Reward: {sample['reward']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nPoor RM Policy - Sample Responses:\")\n",
        "for i, sample in enumerate(poor_results['sample_responses'], 1):\n",
        "    print(f\"  {i}. Prompt: {sample['prompt']}\")\n",
        "    print(f\"     Response: {sample['response']}\")\n",
        "    print(f\"     Reward: {sample['reward']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nGood RM Policy - Sample Responses:\")\n",
        "for i, sample in enumerate(good_results['sample_responses'], 1):\n",
        "    print(f\"  {i}. Prompt: {sample['prompt']}\")\n",
        "    print(f\"     Response: {sample['response']}\")\n",
        "    print(f\"     Reward: {sample['reward']:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp7hn_WGGG3L"
      },
      "source": [
        "## Part 8: Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GOWW6-OH3I"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12)) # Changed to 2x2 grid\n",
        "\n",
        "# 1. Training rewards comparison (with smoothing)\n",
        "window = 10\n",
        "def smooth(data, window_size=window):\n",
        "    if len(data) < window_size:\n",
        "        return data\n",
        "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "if len(poor_rewards) > window:\n",
        "    smooth_poor = smooth(poor_rewards)\n",
        "    smooth_good = smooth(good_rewards)\n",
        "\n",
        "    axes[0, 0].plot(smooth_poor, label='Poor RM', color='red', alpha=0.7, linewidth=2)\n",
        "    axes[0, 0].plot(smooth_good, label='Good RM', color='green', alpha=0.7, linewidth=2)\n",
        "    axes[0, 0].set_title('PPO Training Rewards (Smoothed)', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Average Reward')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. KL divergence during training\n",
        "if len(poor_kls) > window:\n",
        "    smooth_poor_kl = smooth(poor_kls)\n",
        "    smooth_good_kl = smooth(good_kls)\n",
        "\n",
        "    axes[0, 1].plot(smooth_poor_kl, label='Poor RM', color='red', alpha=0.7, linewidth=2)\n",
        "    axes[0, 1].plot(smooth_good_kl, label='Good RM', color='green', alpha=0.7, linewidth=2)\n",
        "    axes[0, 1].set_title('KL Divergence During Training (Smoothed)', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('KL Divergence')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Final performance comparison on FULL EVALUATION SET (now axes[1,0])\n",
        "final_scores = [base_results['mean'], sft_results['mean'], poor_results['mean'], good_results['mean']]\n",
        "score_stds = [base_results['std'], sft_results['std'], poor_results['std'], good_results['std']]\n",
        "labels = ['Base', 'SFT', 'Poor RM\\nPolicy', 'Good RM\\nPolicy']\n",
        "colors = ['gray', 'blue', 'red', 'green']\n",
        "\n",
        "x_pos = np.arange(len(labels))\n",
        "bars = axes[1, 0].bar(x_pos, final_scores, yerr=score_stds,\n",
        "                      color=colors, alpha=0.7, capsize=5, error_kw={'elinewidth': 2})\n",
        "axes[1, 0].set_title(f'Final Model Performance\\n(Evaluated on {len(evaluation_prompts)} Evaluation, using GOOD Reward Model)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Average Reward')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(labels)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].set_ylabel('Reward Value') # Kept this as it clarifies the y-axis for the bar chart.\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add mean and std annotations\n",
        "for i, (bar, score, std) in enumerate(zip(bars, final_scores, score_stds)):\n",
        "    height = bar.get_height()\n",
        "    axes[1, 0].text(i, height + std + 0.02, f'{score:.3f}\\u00c2\\u00b1{std:.3f}',\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Reward distribution boxplot (now axes[1,1])\n",
        "all_rewards = [\n",
        "    base_results['all_rewards'],\n",
        "    sft_results['all_rewards'],\n",
        "    poor_results['all_rewards'],\n",
        "    good_results['all_rewards']\n",
        "]\n",
        "bp = axes[1, 1].boxplot(all_rewards, tick_labels=labels, patch_artist=True)\n",
        "colors_box = ['gray', 'blue', 'red', 'green']\n",
        "for patch, color in zip(bp['boxes'], colors_box):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "axes[1, 1].set_title(f'Reward Distribution\\n(Evaluated on {len(evaluation_prompts)} Evaluation Prompts, using GOOD Reward Model)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Reward')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doNFBK0sNdME"
      },
      "source": [
        "## **(*) TODO 15**\n",
        "## Part 9: Evaluation using the HELD-OUT test prompts\n",
        "\n",
        "Make up three (3) additional prompts by yourself.  Present each one to all four models (base, sft, poorRM, goodRM) and show the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Lx3sbejWRh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
