{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6puMM88Cbxb6"
   },
   "source": [
    "## CSC 580 AI II (Winter 2026) **HW\\#4 Atari Pong** -- Start-up code\n",
    "### **Pong_train.ipynb** -- Perform further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20080,
     "status": "ok",
     "timestamp": 1768759930683,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "29bJ5Pk2hzgL",
    "outputId": "55be29b6-1665-494d-ad14-795d9abced84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# ## Code piece to mount my Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\") # my Google Drive root directory will be mapped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1419,
     "status": "ok",
     "timestamp": 1768759939504,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "qV2p-JvSh0WV",
    "outputId": "5f5c02ba-0234-4146-c89e-d879af7a70ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/CSC580_Winter2026/Atari_Pong\n"
     ]
    }
   ],
   "source": [
    "# # Change the working directory to your own work directory (where the code file is).\n",
    "# import os\n",
    "# thisdir = '/content/drive/My Drive/CSC580_Winter2026/Atari_Pong'\n",
    "# os.chdir(thisdir)\n",
    "\n",
    "# # Ensure the files are there (in the folder)\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spxoFbjeqz1u"
   },
   "source": [
    "## Install relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5572,
     "status": "ok",
     "timestamp": 1768759949035,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "QAVJfLxJqkX0",
    "outputId": "a9e591ae-942b-4c47-95c0-fdacde222fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale_py in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.11.2)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.12.0.88)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.9.0+cu126)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.20.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8jk90gLYyawq"
   },
   "outputs": [],
   "source": [
    "import dqn_core # dqn_core.py\n",
    "from dqn_core import AtariPreprocess, FrameStack, PongActionReducer, ReplayBuffer, DQN, make_env\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Rgcpboq6LT"
   },
   "source": [
    "## Create an ale  (for display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v3JBxeY3nqVp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo_fSrlw2tjk"
   },
   "source": [
    "## 1. Create a DQN model (prediction/q_network and target_network) and load the pre-trained weights.  Also create the replay buffer and load the saved transitions.\n",
    "\n",
    "Assume the q_net weight file is called **\"q_net_XXXX.pt\"** (where XXXX is the number of training steps, e.g. '700k' and '1M'), and it is found under the folder \"checkpoints\".  Note that this code is the same as the one in the evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXFp9t0Ni2Wn"
   },
   "source": [
    "### Functions to load pretrained weights and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6JRiwhErB0fU"
   },
   "outputs": [],
   "source": [
    "def build_networks(n_actions, device):\n",
    "    \"\"\"Build the prediction and target networks.\"\"\"\n",
    "    q_net = DQN(n_actions).to(device)      # prediction network\n",
    "    target_net = DQN(n_actions).to(device) # (frozen) target network\n",
    "    return q_net, target_net\n",
    "\n",
    "def load_pretrained(q_net, target_net, q_file, t_file, device):\n",
    "    \"\"\"Load pretrained weights from files for q_net and target_net.\n",
    "       Move the tensors to device when loading.\"\"\"\n",
    "    q_net.load_state_dict(torch.load(q_file, map_location=device))\n",
    "    target_net.load_state_dict(torch.load(t_file, map_location=device))\n",
    "    target_net.eval() # target network does not 'learn'\n",
    "    print(f\"✅ Loaded pretrained networks from checkpoints\")\n",
    "\n",
    "def load_replay_buffer(pklfilepath):\n",
    "    \"\"\"Load replay buffer.  Assuming a .pkl file.\"\"\"\n",
    "    replay = ReplayBuffer.load(pklfilepath)\n",
    "    #with open(pklfilepath, \"rb\") as f:\n",
    "    #    replay = pickle.load(f)\n",
    "    print(f\"✅ Loaded replay buffer with {len(replay)} transitions\")\n",
    "    return replay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuKsc4qjE397"
   },
   "source": [
    "### **Initial** calls to those functions to load models and reply buffer.  Also create an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34168,
     "status": "ok",
     "timestamp": 1768760092533,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "FA_r32_s_z9k",
    "outputId": "4a26c9df-74c4-4a3b-ea24-25f55c4789bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number of actions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jshresth/tmp/ipykernel_4080259/2050306120.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  q_net.load_state_dict(torch.load(q_file, map_location=device))\n",
      "/scratch/jshresth/tmp/ipykernel_4080259/2050306120.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_net.load_state_dict(torch.load(t_file, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded pretrained networks from checkpoints\n",
      "✅ Loaded replay buffer with 100000 transitions\n"
     ]
    }
   ],
   "source": [
    "import torch # define GPU access\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# folder and filenames for pretrained weights and reply buffer\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "# (*) Initial checkpoint info\n",
    "START_STEP = 700_000\n",
    "str_steps = \"_700k\"\n",
    "\n",
    "qnet_file = f\"{checkpoint_dir}/q_net{str_steps}.pt\"\n",
    "target_file = f\"{checkpoint_dir}/target_net{str_steps}.pt\"\n",
    "replay_file = f\"{checkpoint_dir}/replay{str_steps}.pkl\"\n",
    "\n",
    "#------------------------------\n",
    "# create an environment\n",
    "env = make_env() # default env; function in dqn_core.py\n",
    "n_actions = env.action_space.n\n",
    "print(f\"✅ Number of actions: {n_actions}\")\n",
    "\n",
    "# load pretrained model weights\n",
    "q_net, target_net = build_networks(n_actions, DEVICE)\n",
    "load_pretrained(q_net, target_net, qnet_file, target_file, DEVICE)\n",
    "\n",
    "# Define optimizer here, after q_net is instantiated\n",
    "import torch # Ensure torch is imported if not already\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Load the pre-loaded replay buffer\n",
    "buffer = load_replay_buffer(replay_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plOfxk0utEKg"
   },
   "source": [
    "## 2. Further training\n",
    "### 2.0 Declare global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PrMNdocntId0"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters (students may tune)\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100_000\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_FREQ = 10_000\n",
    "LEARNING_STARTS = 50_000\n",
    "TOTAL_STEPS = 300_000\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1_500_000  # divisor to the current steps (e.g. 700,000 / 1,500,000 = 0.467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBk0n1TttPwD"
   },
   "source": [
    "### 2.1 select_action -- Epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "alzpOK-FtTHQ"
   },
   "outputs": [],
   "source": [
    "def select_action(state, current_steps):\n",
    "    ## Implement epsilon-greedy exploration.  It computes the epsilon as\n",
    "    ## current steps divided by EPS_DECAY subtracted from EPS_START, or\n",
    "    ## EPS_END, whichever the largest.\n",
    "    ## Hint: Look at the evaluation section for the code for greedy choice.\n",
    "    \n",
    "    # compute epsilon\n",
    "    epsilon = max(EPS_END, EPS_START - current_steps / EPS_DECAY)\n",
    "\n",
    "    # Epsilon-greedy with random action with probability epsilon else greedy action\n",
    "    if random.random() < epsilon:\n",
    "        # select random action (exploration)\n",
    "        action = random.randrange(n_actions)\n",
    "    else:\n",
    "        # select greedy action (exploitation)\n",
    "        # to not compute the gradient for the action selection\n",
    "        with torch.no_grad():\n",
    "            # since we are using the network to select the action, we convert the state into tensor\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            action = q_net(state_tensor).argmax(1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tDIcDdxuA0M"
   },
   "source": [
    "### 2.2 Optimization (and backprop) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aR__pqetuG9x"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def optimize():\n",
    "    if len(buffer) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    ## Obtain BATCH_SIZE number of transitions from the reply buffer.\n",
    "    ## states, actions, rewards, next_states, dones = ...\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor(states, device=DEVICE)\n",
    "    actions = torch.tensor(actions, device=DEVICE).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, device=DEVICE)\n",
    "    next_states = torch.tensor(next_states, device=DEVICE)\n",
    "    # Convert dones to float32\n",
    "    dones = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # Q(s, a): From the Q-network’s output, pick the Q-value corresponding\n",
    "    # to the action that was actually taken, for each state in the batch.\n",
    "    # 'q_net(states)' where states is of shape (B, 4, 84, 84), returns\n",
    "    # q-values of the form '[[ 1.2, -0.4,  0.7,  3.5], [ 0.1,  0.9, -0.2,  0.4],..'\n",
    "    # 'gather(1,actions)' returns [[3.5], [0.9],..], and squeeze(1) makes\n",
    "    # into a 1-D array [3.5, 0.9,..]\n",
    "    q_values = q_net(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "    # max_a' Q_target(s', a')\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(1)[0]\n",
    "        ## Compute the target values (based on the algorithm).\n",
    "        # using DQN algorithm target = r + gamma * max(Q_target(s', a')) * (1 - done)\n",
    "        # where (1 - done) is 1 if the episode is not done and 0 if the episode is done\n",
    "        target = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = F.smooth_l1_loss(q_values, target)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the 'Ball Hit' counts.  \n",
    "We detect a hit through bright spots (call) in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate the hit-count metric\n",
    "import cv2\n",
    "\n",
    "def detect_ball(obs):\n",
    "    \"\"\"\n",
    "    obs: (84, 84) uint8 grayscale frame\n",
    "    returns (x, y) or None\n",
    "    \"\"\"\n",
    "    # Defensive: ensure uint8 (in case the value was 0-1 normalized already)\n",
    "    if obs.max() <= 1.0:\n",
    "        obs = (obs * 255).astype(np.uint8)\n",
    "\n",
    "    # Threshold bright pixels (200-255)\n",
    "    _, thresh = cv2.threshold(obs, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
    "        thresh, connectivity=8\n",
    "    )\n",
    "\n",
    "    # Skip background (label 0)\n",
    "    candidates = []\n",
    "    for i in range(1, num_labels):\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        cx, cy = centroids[i]\n",
    "\n",
    "        # Ball is small (paddle is tall and large)\n",
    "        if 2 <= area <= 50:\n",
    "            candidates.append((area, cx, cy))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Choose smallest bright object (ball)\n",
    "    _, x, y = min(candidates, key=lambda x: x[0])\n",
    "    return int(x), int(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXQ8_s5CvbJr"
   },
   "source": [
    "## 3. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAqx-8I_vkg8"
   },
   "outputs": [],
   "source": [
    "def do_train(start_step):\n",
    "    episode_reward = 0\n",
    "    hit_count = 0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    # FIRE to start Pong\n",
    "    obs, _, _, _, _ = env.step(1)\n",
    "\n",
    "    # initialize the previous x and next x (dx) for tracking the ball\n",
    "    prev_x = None\n",
    "    prev_dx = None\n",
    "\n",
    "    for step in range(start_step + 1, start_step + TOTAL_STEPS):\n",
    "        if step%1000 == 0:\n",
    "          print (f\"Step {step}\")\n",
    "\n",
    "        ## Select action based on the epsilon-greedy policy.\n",
    "        action = select_action(obs, step)\n",
    "\n",
    "        # take the action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        ## Store transition in replay buffer\n",
    "        buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "        # from Pong_eval.ipynb count_paddle_hits() function\n",
    "        ball = detect_ball(next_obs[-1]) # find the ball in the last frame\n",
    "        # changed the logic so that it can excecte the code outside of the loop\n",
    "        # even if the ball is not detected in the last frame unlike the original code\n",
    "        if ball is not None:\n",
    "            x, y = ball  # center coordinate of the ball\n",
    "            if prev_x is not None:\n",
    "                dx = x - prev_x  # horizontal displacement of the ball between frames.\n",
    "                                # dx > 0 -- ball moving right, dx < 0 -- ball moving left\n",
    "\n",
    "                # RIGHT paddle hit detection\n",
    "                if prev_dx is not None:  # previous velocity estimate\n",
    "                    # Detect right-paddle hit via velocity reversal.\n",
    "                    # ball previously moving right, now moving left (and the x-position)\n",
    "                    # is close to the right edge of the screen (though 40 <= x <= 65).\n",
    "                    if prev_dx > 0 and dx < 0 and 40 <= x <= 65:\n",
    "                        # user paddle hit the ball!!!\n",
    "                        hit_count += 1\n",
    "\n",
    "                prev_dx = dx\n",
    "            prev_x = x\n",
    "\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "\n",
    "        #if step > LEARNING_STARTS:\n",
    "        optimize()\n",
    "\n",
    "        ## Periodically update target network (TARGET_UPDATE_FREQ).\n",
    "        if step % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "            print(f\"Step {step}: Updated target network\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Step {step}: episode reward = {episode_reward}\")\n",
    "            print(f\"Hit_count = {hit_count}\")\n",
    "\n",
    "            # reset the environment\n",
    "            obs, info = env.reset()\n",
    "            # FIRE to start Pong\n",
    "            obs, _, _, _, _ = env.step(1)\n",
    "\n",
    "            # reset the episode variables\n",
    "            episode_reward = 0\n",
    "            hit_count = 0\n",
    "            prev_x = None\n",
    "            prev_dx = None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOjf3ZpLdddL"
   },
   "source": [
    "## 3.1 **Run do_train()** by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MG_COIsBX0RF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 701000\n",
      "Step 702000\n",
      "Step 703000\n",
      "Step 703790: episode reward = -20.0\n",
      "Step 704000\n",
      "Step 705000\n",
      "Step 706000\n",
      "Step 707000\n",
      "Step 708000\n",
      "Step 708641: episode reward = -21.0\n",
      "Step 709000\n",
      "Step 710000\n",
      "Step 710000: Updated target network\n",
      "Step 711000\n",
      "Step 712000\n",
      "Step 713000\n",
      "Step 713301: episode reward = -18.0\n",
      "Step 714000\n",
      "Step 715000\n",
      "Step 716000\n",
      "Step 717000\n",
      "Step 718000\n",
      "Step 718150: episode reward = -18.0\n",
      "Step 719000\n",
      "Step 720000\n",
      "Step 720000: Updated target network\n",
      "Step 721000\n",
      "Step 722000\n",
      "Step 722480: episode reward = -20.0\n",
      "Step 723000\n",
      "Step 724000\n",
      "Step 725000\n",
      "Step 726000\n",
      "Step 727000\n",
      "Step 727199: episode reward = -21.0\n",
      "Step 728000\n",
      "Step 729000\n",
      "Step 730000\n",
      "Step 730000: Updated target network\n",
      "Step 731000\n",
      "Step 731330: episode reward = -21.0\n",
      "Step 732000\n",
      "Step 733000\n",
      "Step 734000\n",
      "Step 735000\n",
      "Step 735338: episode reward = -21.0\n",
      "Step 736000\n",
      "Step 737000\n",
      "Step 738000\n",
      "Step 739000\n",
      "Step 739732: episode reward = -20.0\n",
      "Step 740000\n",
      "Step 740000: Updated target network\n",
      "Step 741000\n",
      "Step 742000\n",
      "Step 743000\n",
      "Step 744000\n",
      "Step 744118: episode reward = -19.0\n",
      "Step 745000\n",
      "Step 746000\n",
      "Step 747000\n",
      "Step 748000\n",
      "Step 749000\n",
      "Step 749251: episode reward = -19.0\n",
      "Step 750000\n",
      "Step 750000: Updated target network\n",
      "Step 751000\n",
      "Step 752000\n",
      "Step 753000\n",
      "Step 754000\n",
      "Step 755000\n",
      "Step 755587: episode reward = -18.0\n",
      "Step 756000\n",
      "Step 757000\n",
      "Step 758000\n",
      "Step 759000\n",
      "Step 760000\n",
      "Step 760000: Updated target network\n",
      "Step 760391: episode reward = -20.0\n",
      "Step 761000\n",
      "Step 762000\n",
      "Step 763000\n",
      "Step 764000\n",
      "Step 764789: episode reward = -20.0\n",
      "Step 765000\n",
      "Step 766000\n",
      "Step 767000\n",
      "Step 768000\n",
      "Step 769000\n",
      "Step 769680: episode reward = -19.0\n",
      "Step 770000\n",
      "Step 770000: Updated target network\n",
      "Step 771000\n",
      "Step 772000\n",
      "Step 773000\n",
      "Step 774000\n",
      "Step 774147: episode reward = -19.0\n",
      "Step 775000\n",
      "Step 776000\n",
      "Step 777000\n",
      "Step 778000\n",
      "Step 778754: episode reward = -21.0\n",
      "Step 779000\n",
      "Step 780000\n",
      "Step 780000: Updated target network\n",
      "Step 781000\n",
      "Step 782000\n",
      "Step 783000\n",
      "Step 783020: episode reward = -20.0\n",
      "Step 784000\n",
      "Step 785000\n",
      "Step 786000\n",
      "Step 786315: episode reward = -21.0\n",
      "Step 787000\n",
      "Step 788000\n",
      "Step 789000\n",
      "Step 790000\n",
      "Step 790000: Updated target network\n",
      "Step 791000\n",
      "Step 791119: episode reward = -17.0\n",
      "Step 792000\n",
      "Step 793000\n",
      "Step 794000\n",
      "Step 795000\n",
      "Step 795831: episode reward = -18.0\n",
      "Step 796000\n",
      "Step 797000\n",
      "Step 798000\n",
      "Step 799000\n",
      "Step 800000\n",
      "Step 800000: Updated target network\n",
      "Step 800225: episode reward = -20.0\n",
      "Step 801000\n",
      "Step 802000\n",
      "Step 803000\n",
      "Step 804000\n",
      "Step 805000\n",
      "Step 805625: episode reward = -18.0\n",
      "Step 806000\n",
      "Step 807000\n",
      "Step 808000\n",
      "Step 809000\n",
      "Step 810000\n",
      "Step 810000: Updated target network\n",
      "Step 811000\n",
      "Step 811147: episode reward = -17.0\n",
      "Step 812000\n",
      "Step 813000\n",
      "Step 814000\n",
      "Step 815000\n",
      "Step 815045: episode reward = -20.0\n",
      "Step 816000\n",
      "Step 817000\n",
      "Step 818000\n",
      "Step 818327: episode reward = -21.0\n",
      "Step 819000\n",
      "Step 820000\n",
      "Step 820000: Updated target network\n",
      "Step 821000\n",
      "Step 822000\n",
      "Step 822661: episode reward = -20.0\n",
      "Step 823000\n",
      "Step 824000\n",
      "Step 825000\n",
      "Step 826000\n",
      "Step 827000\n",
      "Step 827404: episode reward = -21.0\n",
      "Step 828000\n",
      "Step 829000\n",
      "Step 830000\n",
      "Step 830000: Updated target network\n",
      "Step 831000\n",
      "Step 831589: episode reward = -20.0\n",
      "Step 832000\n",
      "Step 833000\n",
      "Step 834000\n",
      "Step 835000\n",
      "Step 835592: episode reward = -21.0\n",
      "Step 836000\n",
      "Step 837000\n",
      "Step 838000\n",
      "Step 839000\n",
      "Step 839994: episode reward = -20.0\n",
      "Step 840000\n",
      "Step 840000: Updated target network\n",
      "Step 841000\n",
      "Step 842000\n",
      "Step 843000\n",
      "Step 844000\n",
      "Step 845000\n",
      "Step 845434: episode reward = -19.0\n",
      "Step 846000\n",
      "Step 847000\n",
      "Step 848000\n",
      "Step 849000\n",
      "Step 849704: episode reward = -20.0\n",
      "Step 850000\n",
      "Step 850000: Updated target network\n",
      "Step 851000\n",
      "Step 852000\n",
      "Step 853000\n",
      "Step 854000\n",
      "Step 855000\n",
      "Step 855601: episode reward = -19.0\n",
      "Step 856000\n",
      "Step 857000\n",
      "Step 858000\n",
      "Step 859000\n",
      "Step 860000\n",
      "Step 860000: Updated target network\n",
      "Step 860682: episode reward = -18.0\n",
      "Step 861000\n",
      "Step 862000\n",
      "Step 863000\n",
      "Step 864000\n",
      "Step 865000\n",
      "Step 865256: episode reward = -20.0\n",
      "Step 866000\n",
      "Step 867000\n",
      "Step 868000\n",
      "Step 869000\n",
      "Step 870000\n",
      "Step 870000: Updated target network\n",
      "Step 870806: episode reward = -19.0\n",
      "Step 871000\n",
      "Step 872000\n",
      "Step 873000\n",
      "Step 874000\n",
      "Step 874925: episode reward = -21.0\n",
      "Step 875000\n",
      "Step 876000\n",
      "Step 877000\n",
      "Step 878000\n",
      "Step 879000\n",
      "Step 880000\n",
      "Step 880000: Updated target network\n",
      "Step 880463: episode reward = -20.0\n",
      "Step 881000\n",
      "Step 882000\n",
      "Step 883000\n",
      "Step 884000\n",
      "Step 885000\n",
      "Step 885143: episode reward = -20.0\n",
      "Step 886000\n",
      "Step 887000\n",
      "Step 888000\n",
      "Step 889000\n",
      "Step 890000\n",
      "Step 890000: Updated target network\n",
      "Step 891000\n",
      "Step 891407: episode reward = -18.0\n",
      "Step 892000\n",
      "Step 893000\n",
      "Step 894000\n",
      "Step 895000\n",
      "Step 895321: episode reward = -20.0\n",
      "Step 896000\n",
      "Step 897000\n",
      "Step 898000\n",
      "Step 899000\n",
      "Step 900000\n",
      "Step 900000: Updated target network\n",
      "Step 901000\n",
      "Step 901677: episode reward = -20.0\n",
      "Step 902000\n",
      "Step 903000\n",
      "Step 904000\n",
      "Step 905000\n",
      "Step 905932: episode reward = -21.0\n",
      "Step 906000\n",
      "Step 907000\n",
      "Step 908000\n",
      "Step 909000\n",
      "Step 910000\n",
      "Step 910000: Updated target network\n",
      "Step 911000\n",
      "Step 912000\n",
      "Step 913000\n",
      "Step 913044: episode reward = -19.0\n",
      "Step 914000\n",
      "Step 915000\n",
      "Step 916000\n",
      "Step 917000\n",
      "Step 918000\n",
      "Step 919000\n",
      "Step 919008: episode reward = -20.0\n",
      "Step 920000\n",
      "Step 920000: Updated target network\n",
      "Step 921000\n",
      "Step 922000\n",
      "Step 923000\n",
      "Step 924000\n",
      "Step 924321: episode reward = -20.0\n",
      "Step 925000\n",
      "Step 926000\n",
      "Step 927000\n",
      "Step 928000\n",
      "Step 929000\n",
      "Step 930000\n",
      "Step 930000: Updated target network\n",
      "Step 930657: episode reward = -18.0\n",
      "Step 931000\n",
      "Step 932000\n",
      "Step 933000\n",
      "Step 934000\n",
      "Step 935000\n",
      "Step 936000\n",
      "Step 936908: episode reward = -17.0\n",
      "Step 937000\n",
      "Step 938000\n",
      "Step 939000\n",
      "Step 940000\n",
      "Step 940000: Updated target network\n",
      "Step 941000\n",
      "Step 942000\n",
      "Step 942699: episode reward = -19.0\n",
      "Step 943000\n",
      "Step 944000\n",
      "Step 945000\n",
      "Step 946000\n",
      "Step 947000\n",
      "Step 948000\n",
      "Step 948284: episode reward = -20.0\n",
      "Step 949000\n",
      "Step 950000\n",
      "Step 950000: Updated target network\n",
      "Step 951000\n",
      "Step 952000\n",
      "Step 953000\n",
      "Step 953825: episode reward = -19.0\n",
      "Step 954000\n",
      "Step 955000\n",
      "Step 956000\n",
      "Step 957000\n",
      "Step 958000\n",
      "Step 959000\n",
      "Step 960000\n",
      "Step 960000: Updated target network\n",
      "Step 960817: episode reward = -19.0\n",
      "Step 961000\n",
      "Step 962000\n",
      "Step 963000\n",
      "Step 964000\n",
      "Step 965000\n",
      "Step 966000\n",
      "Step 967000\n",
      "Step 967805: episode reward = -19.0\n",
      "Step 968000\n",
      "Step 969000\n",
      "Step 970000\n",
      "Step 970000: Updated target network\n",
      "Step 971000\n",
      "Step 972000\n",
      "Step 973000\n",
      "Step 973869: episode reward = -19.0\n",
      "Step 974000\n",
      "Step 975000\n",
      "Step 976000\n",
      "Step 977000\n",
      "Step 978000\n",
      "Step 979000\n",
      "Step 979833: episode reward = -17.0\n",
      "Step 980000\n",
      "Step 980000: Updated target network\n",
      "Step 981000\n",
      "Step 982000\n",
      "Step 983000\n",
      "Step 984000\n",
      "Step 985000\n",
      "Step 985539: episode reward = -19.0\n",
      "Step 986000\n",
      "Step 987000\n",
      "Step 988000\n",
      "Step 989000\n",
      "Step 990000\n",
      "Step 990000: Updated target network\n",
      "Step 991000\n",
      "Step 991637: episode reward = -18.0\n",
      "Step 992000\n",
      "Step 993000\n",
      "Step 994000\n",
      "Step 995000\n",
      "Step 996000\n",
      "Step 997000\n",
      "Step 998000\n",
      "Step 999000\n"
     ]
    }
   ],
   "source": [
    "# Train further TOTAL_STEPS number of steps\n",
    "do_train(START_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWG30eCddfAF"
   },
   "source": [
    "## 4. Visualize the further-trained model (one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SPbErATuX0SN"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def visualize_one_episode(model, env):\n",
    "    obs, info = env.reset()\n",
    "    obs, _, _, _, _ = env.step(1)  # FIRE to start the game\n",
    "\n",
    "    # set up the visualization variables (fig, img, ax)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    img = ax.imshow(env.render())\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    display_handle = display(fig, display_id=True) # set up display_handle\n",
    "\n",
    "    frames = [] # collect frames for a video\n",
    "\n",
    "    # (*) start an episode (until either player reaches 21 points)\n",
    "    # actions were chosen by the greedy strategy, selecting the action with the\n",
    "    # largest value produced on the output layer of the model (q_net).\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            # tranform obs (4, 84, 84) to a batch (of one instance)\n",
    "            state = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "            # (*) select action with the highest Q-value for the current state.\n",
    "            action = model(state).argmax(1).item()\n",
    "            # q_net(state) returns model output, which is a tensor containing\n",
    "            # values of the actions, e.g. 'tensor([[ 1.2, -0.4,  0.7,  3.5]])',\n",
    "            # where the inner list is the average rewards of the actions.\n",
    "            # Then .argmax(1) returns 'tensor([3])', then by .item(), we get 3,\n",
    "            # the action number of the 'best' action.\n",
    "\n",
    "        # take the action and receive info from the environment\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # visualization\n",
    "        frame = env.render()\n",
    "        frames.append\n",
    "        img.set_data(frame)\n",
    "        display_handle.update(fig)\n",
    "\n",
    "        time.sleep(0.03)\n",
    "\n",
    "    plt.close(fig)\n",
    "    env.close()\n",
    "\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    return frames, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 280238,
     "status": "ok",
     "timestamp": 1768765713420,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "wibTXsdLeOmh",
    "outputId": "9925ef0b-e5a1-41ea-b8b2-a008ddaf5e28"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(q_net\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 8\u001b[0m frames, total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mvisualize_one_episode\u001b[0;34m(model, env)\u001b[0m\n\u001b[1;32m     45\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m     46\u001b[0m     img\u001b[38;5;241m.\u001b[39mset_data(frame)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mdisplay_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.03\u001b[39m)\n\u001b[1;32m     51\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/display_functions.py:374\u001b[0m, in \u001b[0;36mDisplayHandle.update\u001b[0;34m(self, obj, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update existing displays with my id\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        additional keyword arguments passed to update_display\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     \u001b[43mupdate_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/display_functions.py:326\u001b[0m, in \u001b[0;36mupdate_display\u001b[0;34m(obj, display_id, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update an existing display by id\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m:func:`display`\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/decorator.py:235\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    234\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backend_bases.py:2149\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m layout_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mget_layout_engine()\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2146\u001b[0m     \u001b[38;5;66;03m# we need to trigger a draw before printing to make sure\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;66;03m# CL works.  \"tight\" also needs a draw to get the right\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;66;03m# locations:\u001b[39;00m\n\u001b[0;32m-> 2149\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_renderer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backend_bases.py:1574\u001b[0m, in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   1572\u001b[0m         figure\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39m_switch_canvas_and_return_print_method(fmt))\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Done \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1576\u001b[0m     renderer, \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backend_bases.py:2042\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2041\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2042\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:429\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    431\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    432\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:378\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_renderer()\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    381\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n",
      "File \u001b[0;32m/scratch/jshresth/conda/envs/rl/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:269\u001b[0m, in \u001b[0;36mRendererAgg.clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclear\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the visualization function (with the last model/q_net)\n",
    "env = make_env(render_mode=\"rgb_array\") # for visual rendering\n",
    "\n",
    "model = DQN(n_actions)\n",
    "model.load_state_dict(q_net.state_dict())\n",
    "model.eval()\n",
    "\n",
    "frames, total_reward = visualize_one_episode(q_net, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Tp_JfywC9o"
   },
   "source": [
    "## 5. Save the model (q_net and target_net) and the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIR6JzjBv57c"
   },
   "outputs": [],
   "source": [
    "steps = \"_1M\"\n",
    "qnet_file = f\"{checkpoint_dir}/q_net{steps}.pt\"\n",
    "target_file = f\"{checkpoint_dir}/target_net{steps}.pt\"\n",
    "replay_file = f\"{checkpoint_dir}/replay{steps}.pkl\"\n",
    "\n",
    "torch.save(q_net.state_dict(), qnet_file)\n",
    "torch.save(target_net.state_dict(), target_file)\n",
    "buffer.save(replay_file)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7RgfwF0k8sC"
   },
   "source": [
    "## 6. Create a video and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1768765802303,
     "user": {
      "displayName": "Noriko T",
      "userId": "06082512421306527471"
     },
     "user_tz": 360
    },
    "id": "I8hefIQDv6N7",
    "outputId": "3bd91198-d68c-42af-a9d3-3815530f3bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video to videos/pong_1M.mp4\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "video_dir = \"videos\"\n",
    "video_path = f\"{video_dir}/pong{steps}.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=30)\n",
    "print(f\"Saved video to {video_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMgI6h5OF8tNHh76DSmxEos",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
